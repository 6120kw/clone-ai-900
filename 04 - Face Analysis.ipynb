{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 얼굴 감지 및 분석\n",
    "\n",
    "Computer Vision 솔루션은 종종 사람의 얼굴을 감지, 분석, 식별할 수 있는 AI(인공 지능) 솔루션을 필요로 합니다. 예를 들어 소매업체 Northwind Traders가 AI 서비스를 통해 매장을 모니터링하여 도움이 필요한 고객을 파악하고 직원을 보내 돕도록 하는 \"스마트 스토어\"를 구현하기로 결정했다고 가정해 보겠습니다. 이를 달성하기 위한 한 가지 방법은 얼굴 감지 및 분석을 수행하는 것입니다. 즉, 이미지에 얼굴이 있는지 확인하고, 얼굴이 있다면 그 특징을 분석하는 것입니다.\n",
    "\n",
    "![얼굴을 분석하는 로봇](./images/face_analysis.jpg)\n",
    "\n",
    "## Face Cognitive Service를 사용하여 얼굴 감지\n",
    "\n",
    "Northwind Traders가 만들고자 하는 스마트 스토어 시스템이 고객을 감지하고 고객의 얼굴 특징을 분석할 수 있어야 한다고 가정해 보세요. Microsoft Azure에서는 Azure Cognitive Services에 포함된 **Face**를 사용하여 이 작업을 수행할 수 있습니다.\n",
    "\n",
    "### Cognitive Services 리소스 만들기\n",
    "\n",
    "먼저 Azure 구독에서 **Cognitive Services** 리소스를 만들어 보겠습니다.\n",
    "\n",
    "> **참고**: 이미 Cognitive Services 리소스를 보유하고 있다면 Azure Portal에서 **빠른 시작** 페이지를 열고 키 및 엔드포인트를 아래의 셀로 복사하기만 하면 됩니다. 리소스가 없다면 아래의 단계를 따라 리소스를 만듭니다.\n",
    "\n",
    "1. 다른 브라우저 탭에서 Azure Portal (https://portal.azure.com) 을 열고 Microsoft 계정으로 로그인합니다.\n",
    "2. **&#65291;리소스 만들기** 단추를 클릭하고, *Cognitive Services*를 검색하고, 다음 설정을 사용하여 **Cognitive Services** 리소스를 만듭니다.\n",
    "    - **구독**: *사용자의 Azure 구독*.\n",
    "    - **리소스 그룹**: *고유한 이름의 새 리소스 그룹을 선택하거나 만듭니다*.\n",
    "    - **지역**: *사용 가능한 지역을 선택합니다*.\n",
    "    - **이름**: *고유한 이름을 입력합니다*.\n",
    "    - **가격 책정 계층**: S0\n",
    "    - **알림을 읽고 이해했음을 확인합니다**. 선택됨.\n",
    "3. 배포가 완료될 때까지 기다립니다. 그런 다음에 Cognitive Services 리소스로 이동하고, **개요** 페이지에서 링크를 클릭하여 서비스의 키를 관리합니다. 클라이언트 애플리케이션에서 Cognitive Services 리소스에 연결하려면 엔드포인트 및 키가 필요합니다.\n",
    "\n",
    "### Cognitive Services 리소스의 키 및 엔드포인트 가져오기\n",
    "\n",
    "Cognitive Services 리소스를 사용하려면 클라이언트 애플리케이션에 해당 엔드포인트 및 인증 키가 필요합니다.\n",
    "\n",
    "1. Azure Portal에 있는 Cognitive Service 리소스의 **키 및 엔드포인트** 페이지에서 리소스의 **Key1**을 복사하고 아래 코드에 붙여 넣어 **YOUR_COG_KEY**를 대체합니다.\n",
    "\n",
    "2. 리소스의 **엔드포인트**를 복사하고 아래 코드에 붙여 넣어 **YOUR_COG_ENDPOINT**를 대체합니다.\n",
    "\n",
    "3. 셀 실행 <span>&#9655;</span> 단추(셀 왼쪽 상단에 있음)를 클릭하여 아래의 셀에 있는 코드를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1599693964655
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use cognitive services at https://ai-900-kwcgs.cognitiveservices.azure.com/ using key 149b9a076c7d48dd98f306cec2e540c4\n"
     ]
    }
   ],
   "source": [
    "cog_key = '149b9a076c7d48dd98f306cec2e540c4'\n",
    "cog_endpoint = 'https://ai-900-kwcgs.cognitiveservices.azure.com/'\n",
    "\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Face 서비스를 사용하여 매장에서 사람 얼굴을 감지할 수 있는 Cognitive Services 리소스가 생겼습니다.\n",
    "\n",
    "아래의 코드 셀을 실행하여 예시를 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1599693970079
    }
   },
   "outputs": [
    {
     "ename": "APIErrorException",
     "evalue": "(InvalidRequest) Invalid request has been sent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIErrorException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimdoyul/coding/ai/clone/clone-ai-900/04 - Face Analysis.ipynb 셀 4\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000003?line=11'>12</a>\u001b[0m image_stream \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(image_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000003?line=13'>14</a>\u001b[0m \u001b[39m# Detect faces\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000003?line=14'>15</a>\u001b[0m detected_faces \u001b[39m=\u001b[39m face_client\u001b[39m.\u001b[39;49mface\u001b[39m.\u001b[39;49mdetect_with_stream(image\u001b[39m=\u001b[39;49mimage_stream)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000003?line=16'>17</a>\u001b[0m \u001b[39m# Display the faces (code in python_code/faces.py)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000003?line=17'>18</a>\u001b[0m faces\u001b[39m.\u001b[39mshow_faces(image_path, detected_faces)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/azure/cognitiveservices/vision/face/operations/_face_operations.py:782\u001b[0m, in \u001b[0;36mFaceOperations.detect_with_stream\u001b[0;34m(self, image, return_face_id, return_face_landmarks, return_face_attributes, recognition_model, return_recognition_model, detection_model, face_id_time_to_live, custom_headers, raw, callback, **operation_config)\u001b[0m\n\u001b[1;32m    779\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39msend(request, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moperation_config)\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n\u001b[0;32m--> 782\u001b[0m     \u001b[39mraise\u001b[39;00m models\u001b[39m.\u001b[39mAPIErrorException(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize, response)\n\u001b[1;32m    784\u001b[0m deserialized \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "\u001b[0;31mAPIErrorException\u001b[0m: (InvalidRequest) Invalid request has been sent."
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from python_code import faces\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a face detection client.\n",
    "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "\n",
    "# Display the faces (code in python_code/faces.py)\n",
    "faces.show_faces(image_path, detected_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "감지된 얼굴 각각에 고유한 ID가 할당되므로 애플리케이션은 감지된 각각의 얼굴을 식별할 수 있습니다.\n",
    "\n",
    "아래의 셀을 실행하여 좀 더 많은 쇼핑객 얼굴의 ID를 표시해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1599693970447
    }
   },
   "outputs": [
    {
     "ename": "APIErrorException",
     "evalue": "(InvalidRequest) Invalid request has been sent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIErrorException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimdoyul/coding/ai/clone/clone-ai-900/04 - Face Analysis.ipynb 셀 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000005?line=2'>3</a>\u001b[0m image_stream \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(image_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000005?line=4'>5</a>\u001b[0m \u001b[39m# Detect faces\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000005?line=5'>6</a>\u001b[0m detected_faces \u001b[39m=\u001b[39m face_client\u001b[39m.\u001b[39;49mface\u001b[39m.\u001b[39;49mdetect_with_stream(image\u001b[39m=\u001b[39;49mimage_stream)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000005?line=7'>8</a>\u001b[0m \u001b[39m# Display the faces (code in python_code/faces.py)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000005?line=8'>9</a>\u001b[0m faces\u001b[39m.\u001b[39mshow_faces(image_path, detected_faces, show_id\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/azure/cognitiveservices/vision/face/operations/_face_operations.py:782\u001b[0m, in \u001b[0;36mFaceOperations.detect_with_stream\u001b[0;34m(self, image, return_face_id, return_face_landmarks, return_face_attributes, recognition_model, return_recognition_model, detection_model, face_id_time_to_live, custom_headers, raw, callback, **operation_config)\u001b[0m\n\u001b[1;32m    779\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39msend(request, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moperation_config)\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n\u001b[0;32m--> 782\u001b[0m     \u001b[39mraise\u001b[39;00m models\u001b[39m.\u001b[39mAPIErrorException(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize, response)\n\u001b[1;32m    784\u001b[0m deserialized \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "\u001b[0;31mAPIErrorException\u001b[0m: (InvalidRequest) Invalid request has been sent."
     ]
    }
   ],
   "source": [
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "\n",
    "# Display the faces (code in python_code/faces.py)\n",
    "faces.show_faces(image_path, detected_faces, show_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 얼굴 특성 분석\n",
    "\n",
    "Face는 얼굴을 감지하는 것 외에도 많은 일들을 할 수 있습니다. 얼굴 특징 및 표현을 분석하여 연령과 감정 상태를 파악할 수도 있습니다. 예를 들어 아래 코드를 실행하여 쇼핑객의 얼굴 특성을 분석해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1599693971321
    }
   },
   "outputs": [
    {
     "ename": "APIErrorException",
     "evalue": "(InvalidRequest) Invalid request has been sent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIErrorException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimdoyul/coding/ai/clone/clone-ai-900/04 - Face Analysis.ipynb 셀 8\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000007?line=5'>6</a>\u001b[0m \u001b[39m# Detect faces and specified facial attributes\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000007?line=6'>7</a>\u001b[0m attributes \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000007?line=7'>8</a>\u001b[0m detected_faces \u001b[39m=\u001b[39m face_client\u001b[39m.\u001b[39;49mface\u001b[39m.\u001b[39;49mdetect_with_stream(image\u001b[39m=\u001b[39;49mimage_stream, return_face_attributes\u001b[39m=\u001b[39;49mattributes)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000007?line=9'>10</a>\u001b[0m imp\u001b[39m.\u001b[39mrelaed(faces)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000007?line=11'>12</a>\u001b[0m \u001b[39m# Display the faces and attributes (code in python_code/faces.py)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/azure/cognitiveservices/vision/face/operations/_face_operations.py:782\u001b[0m, in \u001b[0;36mFaceOperations.detect_with_stream\u001b[0;34m(self, image, return_face_id, return_face_landmarks, return_face_attributes, recognition_model, return_recognition_model, detection_model, face_id_time_to_live, custom_headers, raw, callback, **operation_config)\u001b[0m\n\u001b[1;32m    779\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39msend(request, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moperation_config)\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n\u001b[0;32m--> 782\u001b[0m     \u001b[39mraise\u001b[39;00m models\u001b[39m.\u001b[39mAPIErrorException(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize, response)\n\u001b[1;32m    784\u001b[0m deserialized \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "\u001b[0;31mAPIErrorException\u001b[0m: (InvalidRequest) Invalid request has been sent."
     ]
    }
   ],
   "source": [
    "# import imp(변경오류 발생 시 실행1)\n",
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam1.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces and specified facial attributes\n",
    "attributes = ['age', 'emotion']\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
    "\n",
    "# imp.relaed(faces)(변경오류 발생 시 실행2)\n",
    "\n",
    "# Display the faces and attributes (code in python_code/faces.py)\n",
    "faces.show_face_attributes(image_path, detected_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지에서 고객에 대해 감지된 감정 점수를 바탕으로 판단할 때, 이 고객은 쇼핑 경험에 꽤 만족한 것처럼 보입니다.\n",
    "\n",
    "## 유사 얼굴 찾기 \n",
    "\n",
    "감지된 각 얼굴에 대해 생성된 얼굴 ID는 얼굴 감지를 개별적으로 식별하는 데 사용됩니다. 이 ID를 사용하여 감지된 얼굴을 이전에 감지된 얼굴과 비교하고 유사한 특징의 얼굴을 찾을 수 있습니다.\n",
    "\n",
    "예를 들어 아래의 셀을 실행하여 한 이미지에 있는 쇼핑객과 다른 이미지에 있는 쇼핑객을 비교하여 일치하는 얼굴을 찾아보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1599693972555
    }
   },
   "outputs": [
    {
     "ename": "APIErrorException",
     "evalue": "(InvalidRequest) Invalid request has been sent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIErrorException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimdoyul/coding/ai/clone/clone-ai-900/04 - Face Analysis.ipynb 셀 10\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000009?line=1'>2</a>\u001b[0m image_1_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mface\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstore_cam3.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000009?line=2'>3</a>\u001b[0m image_1_stream \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(image_1_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000009?line=3'>4</a>\u001b[0m image_1_faces \u001b[39m=\u001b[39m face_client\u001b[39m.\u001b[39;49mface\u001b[39m.\u001b[39;49mdetect_with_stream(image\u001b[39m=\u001b[39;49mimage_1_stream)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000009?line=4'>5</a>\u001b[0m face_1 \u001b[39m=\u001b[39m image_1_faces[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000009?line=6'>7</a>\u001b[0m \u001b[39m# Get the face IDs in a second image\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/azure/cognitiveservices/vision/face/operations/_face_operations.py:782\u001b[0m, in \u001b[0;36mFaceOperations.detect_with_stream\u001b[0;34m(self, image, return_face_id, return_face_landmarks, return_face_attributes, recognition_model, return_recognition_model, detection_model, face_id_time_to_live, custom_headers, raw, callback, **operation_config)\u001b[0m\n\u001b[1;32m    779\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39msend(request, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moperation_config)\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n\u001b[0;32m--> 782\u001b[0m     \u001b[39mraise\u001b[39;00m models\u001b[39m.\u001b[39mAPIErrorException(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize, response)\n\u001b[1;32m    784\u001b[0m deserialized \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "\u001b[0;31mAPIErrorException\u001b[0m: (InvalidRequest) Invalid request has been sent."
     ]
    }
   ],
   "source": [
    "# Get the ID of the first face in image 1\n",
    "image_1_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
    "image_1_stream = open(image_1_path, \"rb\")\n",
    "image_1_faces = face_client.face.detect_with_stream(image=image_1_stream)\n",
    "face_1 = image_1_faces[0]\n",
    "\n",
    "# Get the face IDs in a second image\n",
    "image_2_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
    "image_2_stream = open(image_2_path, \"rb\")\n",
    "image_2_faces = face_client.face.detect_with_stream(image=image_2_stream)\n",
    "image_2_face_ids = list(map(lambda face: face.face_id, image_2_faces))\n",
    "\n",
    "# Find faces in image 2 that are similar to the one in image 1\n",
    "similar_faces = face_client.face.find_similar(face_id=face_1.face_id, face_ids=image_2_face_ids)\n",
    "\n",
    "# Show the face in image 1, and similar faces in image 2(code in python_code/face.py)\n",
    "faces.show_similar_faces(image_1_path, face_1, image_2_path, image_2_faces, similar_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def sum(a, b):\n",
    "    return a+b\n",
    "print(sum(1, 2))\n",
    "\n",
    "print((lambda a,b:a+b)(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6]\n",
      "[2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "temp = [1, 2, 3, 4, 5]\n",
    "result = []\n",
    "for x in temp:\n",
    "    result.append(x+1)\n",
    "print(result)\n",
    "\n",
    "def sum(x):\n",
    "    return x+1\n",
    "print(list(map(sum, temp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 얼굴 인식\n",
    "\n",
    "지금까지, Face가 얼굴과 얼굴 특징을 감지할 수 있고 서로 유사한 두 얼굴을 식별할 수 있다는 것을 확인했습니다. 이제 특정인의 얼굴을 인식하도록 Face를 학습시킬 수 있는 *얼굴 인식* 솔루션을 구현함으로써 한 단계 더 나아갈 수 있습니다. 이것은 소셜 미디어 애플리케이션에서 친구의 사진을 자동으로 태깅하거나 얼굴 인식을 생체 인식 확인 시스템의 일부분으로 사용하는 등 다양한 시나리오에서 유용할 수 있습니다.\n",
    "\n",
    "이것이 어떻게 작동하는지 살펴보기 위해 Northwind Traders 회사가 얼굴 인식을 사용하여 IT 부서의 인증된 직원에게만 보안 시스템 액세스를 허용하려고 한다고 가정해 보겠습니다.\n",
    "\n",
    "먼저, 인증된 직원을 나타내는 *사람 그룹*을 만들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1599693973492
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(InvalidRequest) Invalid request has been sent.\n"
     ]
    },
    {
     "ename": "APIErrorException",
     "evalue": "(InvalidRequest) Invalid request has been sent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIErrorException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimdoyul/coding/ai/clone/clone-ai-900/04 - Face Analysis.ipynb 셀 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000011?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(ex\u001b[39m.\u001b[39mmessage)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000011?line=6'>7</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000011?line=7'>8</a>\u001b[0m     face_client\u001b[39m.\u001b[39;49mperson_group\u001b[39m.\u001b[39;49mcreate(group_id, \u001b[39m'\u001b[39;49m\u001b[39memployees\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000011?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mGroup created!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/azure/cognitiveservices/vision/face/operations/_person_group_operations.py:121\u001b[0m, in \u001b[0;36mPersonGroupOperations.create\u001b[0;34m(self, person_group_id, name, user_data, recognition_model, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[1;32m    118\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39msend(request, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moperation_config)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mraise\u001b[39;00m models\u001b[39m.\u001b[39mAPIErrorException(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize, response)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m raw:\n\u001b[1;32m    124\u001b[0m     client_raw_response \u001b[39m=\u001b[39m ClientRawResponse(\u001b[39mNone\u001b[39;00m, response)\n",
      "\u001b[0;31mAPIErrorException\u001b[0m: (InvalidRequest) Invalid request has been sent."
     ]
    }
   ],
   "source": [
    "group_id = 'employee_group_id'\n",
    "try:\n",
    "    # Delete group if it already exists\n",
    "    face_client.person_group.delete(group_id)\n",
    "except Exception as ex:\n",
    "    print(ex.message)\n",
    "finally:\n",
    "    face_client.person_group.create(group_id, 'employees')\n",
    "    print ('Group created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사람 그룹*이 있으면 그룹에 포함하려는 각 직원에 대해 *사람*을 추가한 후에 각각의 사람에 대한 여러 사진을 등록하여 Face가 각자의 얼굴 특징을 학습하도록 할 수 있습니다. 같은 사람을 여러 가지 포즈와 얼굴 표현으로 보여주는 이미지를 사용하는 것이 좋습니다.\n",
    "\n",
    "Wendell이라는 직원을 추가하고 이 직원의 사진을 세 장 등록하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1599693976898
    }
   },
   "outputs": [
    {
     "ename": "APIErrorException",
     "evalue": "(InvalidRequest) Invalid request has been sent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIErrorException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimdoyul/coding/ai/clone/clone-ai-900/04 - Face Analysis.ipynb 셀 16\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000013?line=3'>4</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000013?line=5'>6</a>\u001b[0m \u001b[39m# Add a person (Wendell) to the group\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000013?line=6'>7</a>\u001b[0m wendell \u001b[39m=\u001b[39m face_client\u001b[39m.\u001b[39;49mperson_group_person\u001b[39m.\u001b[39;49mcreate(group_id, \u001b[39m'\u001b[39;49m\u001b[39mWendell\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000013?line=8'>9</a>\u001b[0m \u001b[39m# Get photo's of Wendell\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000013?line=9'>10</a>\u001b[0m folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mface\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwendell\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/azure/cognitiveservices/vision/face/operations/_person_group_person_operations.py:87\u001b[0m, in \u001b[0;36mPersonGroupPersonOperations.create\u001b[0;34m(self, person_group_id, name, user_data, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[1;32m     84\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39msend(request, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moperation_config)\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mraise\u001b[39;00m models\u001b[39m.\u001b[39mAPIErrorException(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize, response)\n\u001b[1;32m     89\u001b[0m deserialized \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "\u001b[0;31mAPIErrorException\u001b[0m: (InvalidRequest) Invalid request has been sent."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Add a person (Wendell) to the group\n",
    "wendell = face_client.person_group_person.create(group_id, 'Wendell')\n",
    "\n",
    "# Get photo's of Wendell\n",
    "folder = os.path.join('data', 'face', 'wendell')\n",
    "wendell_pics = os.listdir(folder)\n",
    "\n",
    "# Register the photos\n",
    "i = 0\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for pic in wendell_pics:\n",
    "    # Add each photo to person in person group\n",
    "    img_path = os.path.join(folder, pic)\n",
    "    img_stream = open(img_path, \"rb\")\n",
    "    face_client.person_group_person.add_face_from_stream(group_id, wendell.person_id, img_stream)\n",
    "\n",
    "    # Display each image\n",
    "    img = Image.open(img_path)\n",
    "    i +=1\n",
    "    a=fig.add_subplot(1,len(wendell_pics), i)\n",
    "    a.axis('off')\n",
    "    imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사람을 추가하고 사진을 등록했으면 이제 Face가 각 사람을 인식하도록 학습시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1599693977046
    }
   },
   "outputs": [
    {
     "ename": "APIErrorException",
     "evalue": "(InvalidRequest) Invalid request has been sent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIErrorException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimdoyul/coding/ai/clone/clone-ai-900/04 - Face Analysis.ipynb 셀 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000015?line=0'>1</a>\u001b[0m face_client\u001b[39m.\u001b[39;49mperson_group\u001b[39m.\u001b[39;49mtrain(group_id)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000015?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrained!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/azure/cognitiveservices/vision/face/operations/_person_group_operations.py:455\u001b[0m, in \u001b[0;36mPersonGroupOperations.train\u001b[0;34m(self, person_group_id, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[1;32m    452\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39msend(request, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moperation_config)\n\u001b[1;32m    454\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m202\u001b[39m]:\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mraise\u001b[39;00m models\u001b[39m.\u001b[39mAPIErrorException(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize, response)\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m raw:\n\u001b[1;32m    458\u001b[0m     client_raw_response \u001b[39m=\u001b[39m ClientRawResponse(\u001b[39mNone\u001b[39;00m, response)\n",
      "\u001b[0;31mAPIErrorException\u001b[0m: (InvalidRequest) Invalid request has been sent."
     ]
    }
   ],
   "source": [
    "face_client.person_group.train(group_id)\n",
    "print('Trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델 학습이 완료되었으므로 이 모델을 사용하여 이미지에서 인식된 얼굴을 식별할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1599693994820
    }
   },
   "outputs": [
    {
     "ename": "APIErrorException",
     "evalue": "(InvalidRequest) Invalid request has been sent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIErrorException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kimdoyul/coding/ai/clone/clone-ai-900/04 - Face Analysis.ipynb 셀 20\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000017?line=1'>2</a>\u001b[0m image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mface\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39memployees.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000017?line=2'>3</a>\u001b[0m image_stream \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(image_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000017?line=3'>4</a>\u001b[0m image_faces \u001b[39m=\u001b[39m face_client\u001b[39m.\u001b[39;49mface\u001b[39m.\u001b[39;49mdetect_with_stream(image\u001b[39m=\u001b[39;49mimage_stream)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000017?line=4'>5</a>\u001b[0m image_face_ids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m face: face\u001b[39m.\u001b[39mface_id, image_faces))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kimdoyul/coding/ai/clone/clone-ai-900/04%20-%20Face%20Analysis.ipynb#ch0000017?line=6'>7</a>\u001b[0m \u001b[39m# Get recognized face names\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/azure/cognitiveservices/vision/face/operations/_face_operations.py:782\u001b[0m, in \u001b[0;36mFaceOperations.detect_with_stream\u001b[0;34m(self, image, return_face_id, return_face_landmarks, return_face_attributes, recognition_model, return_recognition_model, detection_model, face_id_time_to_live, custom_headers, raw, callback, **operation_config)\u001b[0m\n\u001b[1;32m    779\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39msend(request, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moperation_config)\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n\u001b[0;32m--> 782\u001b[0m     \u001b[39mraise\u001b[39;00m models\u001b[39m.\u001b[39mAPIErrorException(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize, response)\n\u001b[1;32m    784\u001b[0m deserialized \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "\u001b[0;31mAPIErrorException\u001b[0m: (InvalidRequest) Invalid request has been sent."
     ]
    }
   ],
   "source": [
    "# Get the face IDs in a second image\n",
    "image_path = os.path.join('data', 'face', 'employees.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "image_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "image_face_ids = list(map(lambda face: face.face_id, image_faces))\n",
    "\n",
    "# Get recognized face names\n",
    "face_names = {}\n",
    "recognized_faces = face_client.face.identify(image_face_ids, group_id)\n",
    "for face in recognized_faces:\n",
    "    person_name = face_client.person_group_person.get(group_id, face.candidates[0].person_id).name\n",
    "    face_names[face.face_id] = person_name\n",
    "\n",
    "# show recognized faces\n",
    "faces.show_recognized_faces(image_path, image_faces, face_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자세한 내용\n",
    "\n",
    "Face Cognitive Service에 대해 자세히 알아보려면 [Face 설명서](https://docs.microsoft.com/azure/cognitive-services/face/)를 참조하세요.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
